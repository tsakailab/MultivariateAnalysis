{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAMCFacnaXJGzX6JkJsRi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/MultivariateAnalysis/blob/main/ipynb/ex_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前処理と変換\n",
        "\n",
        "特徴量（データフレームの列）の大きさを整える代表的なデータ前処理の手法を学びます．これらの前処理は，特徴量のばらつきの幅や分布の中心を調整します．特定の特徴量を重視・軽視し過ぎる偏ったデータ分析や機械学習を防止したり，数値計算を安定にする効果があります．"
      ],
      "metadata": {
        "id": "PSwhMky-TJLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83Jw7DMUE2Vm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- データセットの読み込み ---\n",
        "# https://www.statsmodels.org/stable/datasets/index.html\n",
        "# https://vincentarelbundock.github.io/Rdatasets/datasets.html\n",
        "\n",
        "# 'mtcars': https://rstudio-pubs-static.s3.amazonaws.com/61800_faea93548c6b49cc91cd0c5ef5059894.html\n",
        "df = sm.datasets.get_rdataset('mtcars', 'datasets').data\n",
        "\n",
        "# 'iris': https://rstudio-pubs-static.s3.amazonaws.com/450733_9a472ce9632f4ffbb2d6175aaaee5be6.html\n",
        "#df = sm.datasets.get_rdataset('iris', 'datasets').data\n",
        "\n",
        "# 'wine': https://rstudio-pubs-static.s3.amazonaws.com/969152_310d885fec5f4a86842ac74d5c6929f2.html\n",
        "#from sklearn.datasets import load_wine\n",
        "#data = load_wine()\n",
        "#df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "#df['target'] = data.target\n",
        "\n",
        "# toy example:\n",
        "#df = pd.DataFrame(np.array([[4, 0, 5, 1], [3, 0, 4, 1], [0, 5, 0, 4], [4, 0, 5, 0], [0, 4, 0, 3]]))\n",
        "\n",
        "print(\"Number of data: n =\", len(df))\n",
        "print(\"Available features:\", list(df.columns))"
      ],
      "metadata": {
        "id": "CdwkNZMZE9CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 特徴を選択してください ---\n",
        "selected_feature = ['wt']\n",
        "x = df[selected_feature]\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "_aDyIJc5FJjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 特徴の分布 ---\n",
        "print(x.agg(['min', 'max']))\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.histplot(x, kde=True, legend=False)\n",
        "plt.title(f'Original data distribution (\"{selected_feature}\")')\n",
        "plt.xlabel('Value')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hRbhuapdFJ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 中心化（centering）\n",
        "各特徴量から，その特徴量の標本平均を引きます．中心化した特徴量の標本平均はゼロになります．\n",
        "\n",
        "👍 **長所**：特徴量が原点からとても離れた位置に集中して分布しているとき，大きな平均値からの小さなズレによってデータが表されていることになります．平均値を除去することで，データ固有の小さなズレに着目しやすくなります．  \n",
        "👎 **短所**：データのばらつき（分散）は変わりません．分散に敏感なアルゴリズムを用いる際，中心化だけでは不十分です．  \n",
        "🎯 **使いどころ**：多変量データのばらつきや内訳を調べる主成分分析（PCA），特徴量を正負の両方に分布させたいときなど．  "
      ],
      "metadata": {
        "id": "SbK45UEQS0Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 中心化（centering） ---\n",
        "x_centered = x - x.mean()\n",
        "\n",
        "# 線形写像で計算してみる（非実用的）\n",
        "#n = len(x)\n",
        "#x_centered = (np.eye(n) - np.ones((n,n))/n).dot(x)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.histplot(x, kde=True, ax=axes[0], legend=False).set_title('Before')\n",
        "sns.histplot(x_centered, kde=True, ax=axes[1], facecolor='lightgreen', legend=False).set_title('After (centered)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_0teuNF5MFh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "ちなみに，標本平均は内積で書き表せます．また，中心化は線形写像です．  \n",
        "\n",
        "$n$ 個のデータの特徴量を成分にもつベクトルを $\\boldsymbol{x}=[x^{(1)},\\dots,x^{(n)}]^\\top\\in\\mathbb{R}^{n}$，  \n",
        "すべての成分が 1 の $n$ 次元ベクトルを $\\boldsymbol{1}_n=[1,1,\\dots,1]^\\top\\in\\mathbb{R}^n$（all-ones vector）とすると，  \n",
        "特徴量の標本平均は  \n",
        "$$\\hat\\mu=\\frac{1}{n}\\sum_{i=1}^nx^{(i)}=\\frac{1}{n}\\boldsymbol{1}_n^\\top\\boldsymbol{x}$$\n",
        "のように，$\\boldsymbol{1}_n$ と $\\boldsymbol{x}$ の内積で表せます．$\\top$は転置です．  \n",
        "\n",
        "$\\boldsymbol{x}$ を中心化した結果は\n",
        "$$\\hat{\\boldsymbol{x}} = \\boldsymbol{x} - \\frac{1}{n}(\\boldsymbol{1}_n^\\top \\boldsymbol{x})\\,\\boldsymbol{1}_n$$\n",
        "または\n",
        "$$ \\hat{\\boldsymbol{x}} = \\boldsymbol{x}-\\frac{1}{n}\\boldsymbol{1}_n\\boldsymbol{1}_n^\\top\\boldsymbol{x}=(\\boldsymbol{I}_n-\\frac{1}{n}\\boldsymbol{1}_n\\boldsymbol{1}_n^\\top)\\,\\boldsymbol{x}$$\n",
        "と書き表せます．\n",
        "ただし，$\\boldsymbol{I}_n\\in\\mathbb{R}^{n\\times n}$ は $n$ 次正方の単位行列，$\\boldsymbol{1}_n\\boldsymbol{1}_n^\\top\\in\\mathbb{R}^{n\\times n}$は要素がすべて 1 の $n$ 次正方行列です．\n",
        "\n",
        "ベクトルや行列のサイズに注意しながら，上記の計算が成り立つことを，いちど丁寧に確認するとよいでしょう．\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "ulfPljB-d9Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## スケーリング（scaling）"
      ],
      "metadata": {
        "id": "6P0XEdCrMlK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Min-Maxスケーリング（min-max scaling）\n",
        "各特徴量ごとに，最小値が 0，最大値が 1 になるようにデータを変換します．\n",
        "\n",
        "👍 **長所**：データの範囲を一定に揃える，非常に直感的な手法です．変換後の値が必ず [0, 1] の範囲に収まるため，スケールに依存するアルゴリズム（例：データ間の距離に基づく分類など）を用する際に，簡易的な前処理としてよく用いられます．  \n",
        "👎 **短所**：**外れ値による影響が大きい**です．巨大な外れ値がひとつでもあると，他のほとんどの特徴量が 0 または 1 付近の非常に狭い範囲に押し込められてしまいます．  \n",
        "🎯 **使いどころ**：画像データのピクセル値（0-255を0-1に変換）など，データの範囲が明確に決まっている場合や，外れ値がないことがわかっている場合に有効です．\n",
        "\n",
        "$n$ 個のデータについて，特徴量 $x^{(1)},\\dots,x^{(n)}$ の最小値と最大値をそれぞれ $x_{\\scriptsize\\mbox{min}}=\\min_i x^{(i)}$，$x_{\\scriptsize\\mbox{max}}=\\max_i x^{(i)}$ とします．$x^{(i)}$ をmin-maxスケーリングした結果は\n",
        "$$\\hat x^{(i)} = \\frac{x^{(i)} - x_{\\scriptsize\\mbox{min}}}{x_{\\scriptsize\\mbox{max}} - x_{\\scriptsize\\mbox{min}}}$$\n",
        "と書き表せます．"
      ],
      "metadata": {
        "id": "4Z-_vyBtYThs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- min-max正規化 (min-max scaling) ---\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "# 【実習】sklearn を使わないで x_scaled を作りましょう．\n",
        "# x_min = x.values.min(axis=0)\n",
        "# x_max = x.values.max(axis=0)\n",
        "# x_scaled = ?\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.histplot(x, kde=True, ax=axes[0], legend=False).set_title('Before')\n",
        "sns.histplot(x_scaled, kde=True, ax=axes[1], facecolor='lightgreen', legend=False).set_title('After (min-max scaled)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9HQevr0TFKAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "ベクトル $\\boldsymbol{x}=[x^{(1)},\\dots,x^{(n)}]^\\top\\in\\mathbb{R}^{n}$ を用いると，min-maxスケーリングの結果は次式のように表せます．\n",
        "\n",
        "$$\\hat{\\boldsymbol{x}} =\n",
        "\\frac{\\boldsymbol{x} - x_{\\scriptsize\\mbox{min}}\\boldsymbol{1}_n}{x_{\\scriptsize\\mbox{max}}-x_{\\scriptsize\\mbox{min}}}$$\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "dKpiT2KPowKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 標準化（standardization / z-score）\n",
        "\n",
        "各特徴量の標本平均が 0，標本標準偏差が 1 になるように変換します．\n",
        "\n",
        "👍 **長所**：正規分布を仮定できる特徴量に対して合理的な前処理です．標準化の特徴量は大きさの期待値が1と仮定できます．Min-maxスケーリングよりも外れ値の影響を受けにくいです．  \n",
        "👎 **短所**：変換後の値の範囲は一定ではありません（必ずしも [-1, 1] に収まるわけではありません）．  \n",
        "🎯 **使いどころ**：特徴量の分布が正規分布に近い場合や，どのスケーリング手法を使うか迷った場合に，最も用いられます．\n",
        "\n",
        "\n",
        "$n$ 個のデータの特徴量 $x^{(1)},\\dots,x^{(n)}$ の標本平均と標本分散をそれぞれ $\\hat\\mu$，$\\hat\\sigma^2$ とすると，$x^{(i)}$ を標準化した結果は\n",
        "$$z^{(i)} = \\frac{x^{(i)} - \\hat\\mu}{\\hat\\sigma}$$\n",
        "と書き表せます．"
      ],
      "metadata": {
        "id": "nxxnE7xjNYmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 標準化（standardization）---\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "z = scaler.fit_transform(x)\n",
        "\n",
        "# 【実習】sklearn を使わないで z を作りましょう．x の標本平均と標本標準偏差は x.mean() と x.std() です．\n",
        "# z = ?\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.histplot(x, kde=True, ax=axes[0], legend=False).set_title('Before')\n",
        "sns.histplot(z, kde=True, ax=axes[1], facecolor='lightgreen', legend=False).set_title('After (standardized)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ETewFPfMMFmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "ベクトル $\\boldsymbol{x}=[x^{(1)},\\dots,x^{(n)}]^\\top\\in\\mathbb{R}^{n}$ を用いると，標本平均は $\\hat\\mu=\\frac{1}{n}\\boldsymbol{1}_n^\\top\\boldsymbol{x}$，標本分散は\n",
        "$$\\hat\\sigma^2=\\frac{1}{n}\\sum_{i=1}^n(x^{(i)}-\\hat\\mu)^2=\\frac{1}{n}\\|\\boldsymbol{x}-\\hat\\mu\\boldsymbol{1}_n\\|_2^2$$\n",
        "と書けるので，標準化の結果 $\\boldsymbol{z}=[z^{(1)},\\dots,z^{(n)}]^\\top\\in\\mathbb{R}^{n}$ は次式のように表せます．\n",
        "$$\\boldsymbol{z} =\n",
        "\\frac{\\boldsymbol{x} - \\frac{1}{n}(\\boldsymbol{1}_n^\\top \\boldsymbol{x})\\,\\boldsymbol{1}_n}\n",
        "{\\frac{1}{\\sqrt{n}}\\left\\|\\boldsymbol{x} - \\frac{1}{n}(\\boldsymbol{1}_n^\\top \\boldsymbol{x})\\,\\boldsymbol{1}_n\\right\\|_2}\\;\\in\\mathbb{R}^n$$\n",
        "\n",
        "ただし，$\\|\\boldsymbol{a}\\|_2=\\sqrt{\\boldsymbol{a}^\\top\\boldsymbol{a}}=\\sqrt{\\sum_ka_k^2}$ はベクトルの $\\ell_2$ノルム（$\\ell_2$ norm）です．\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "0NeLwLULejvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ロバストスケーリング（Robust scaling）\n",
        "\n",
        " 中央値を引き，四分位範囲（interquartile range; IQR）で割ります．IQRは，第3四分位数（75パーセンタイル値）から第1四分位数（25パーセンタイル値）を引いたものです．\n",
        "\n",
        "👍 **長所**：平均や標準偏差の代わりに中央値と四分位範囲を使うので，外れ値の影響をほとんど受けません．  \n",
        "👎 **短所**：他の手法に比べてやや計算が複雑です．  \n",
        "🎯 **使いどころ**：データに外れ値が多く含まれていることが分かっている場合に最も効果的です．\n",
        "\n",
        "$n$ 個のデータについて，特徴量 $x^{(1)},\\dots,x^{(n)}$ の第1四分位数，第2四分位数（中央値），第3四分位数をそれぞれ $q_1$，$q_2$，$q_3$ とします．$x^{(i)}$ をロバストスケーリングした結果は\n",
        "$$\\hat x^{(i)} = \\frac{x^{(i)} - q_2}{q_3 - q_1}$$\n",
        "と書き表せます．分母の$q_3-q_1$がIQRです．"
      ],
      "metadata": {
        "id": "ZhRroJHhbS7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ロバストスケーリング（robust scaling）---\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "# sklearn を使わないで x_scaled を作りましょう．\n",
        "#x_scaled = (x - np.median(x)) / (np.percentile(x, 75) - np.percentile(x, 25))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.histplot(x, kde=True, ax=axes[0], legend=False).set_title('Before')\n",
        "sns.histplot(x_scaled, kde=True, ax=axes[1], facecolor='lightgreen', legend=False).set_title('After (robust-scaled)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NZ3O0pAj0PU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 多変量データの前処理\n",
        "\n",
        "複数の特徴量（多変量）でデータが表されているときは，以下の前処理を検討します．\n",
        "* (P) 各特徴量（データフレームの列）ごとにスケーリングする\n",
        "* (N) 各データ点（データフレームの行）ごとに正規化する"
      ],
      "metadata": {
        "id": "4Q357mVo2FVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available features:\", list(df.columns))\n",
        "X = df   # 指定した列だけ処理したければ X = df[['mpg', 'hp', 'wp']] のように抜き出す\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "SGKG0l8e6IXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (P) 各特徴量ごとにスケーリングする"
      ],
      "metadata": {
        "id": "CtLpo7Os2U2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "\n",
        "# 適用したいスケーリングを設定してください\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "yCsmfz-MBibC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2つの特徴量を指定して，スケーリング前後の散布図を観察しましょう\n",
        "# 横軸と縦軸の特徴量を２つ選んでください\n",
        "feats = ['wt', 'mpg']  # ['wt', 'mpg', df['vs'].values] # ３つ目があれば色分けに使う\n",
        "\n",
        "dfXs = [(\"Before\", X[feats[:2]]),\n",
        "        (\"After (scaled)\", pd.DataFrame(X_scaled[:,df.columns.get_indexer(feats[:2])], columns=feats[:2]))]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(dfXs), figsize=(14, 5))\n",
        "fig.subplots_adjust(wspace=0.5)\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "for (_title, _df), _ax in zip(dfXs, axes):\n",
        "    _ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    sns.kdeplot(ax=_ax, data=_df, x=feats[0], y=feats[1], fill=True, alpha=0.1)\n",
        "    sns.scatterplot(ax=_ax, data=_df, x=feats[0], y=feats[1], s=60, hue=feats[2] if len(feats)==3 else None).set_title(_title)\n",
        "    axk = _ax.inset_axes([0, 1.02, 1, 0.2]) # [left, bottom, width, height]\n",
        "    sns.kdeplot(ax=axk, data=_df, x=feats[0], fill=True)\n",
        "    axk.axis('off')\n",
        "    axk = _ax.inset_axes([1.02, 0, 0.2, 1])\n",
        "    sns.kdeplot(ax=axk, data=_df, y=feats[1], fill=True)\n",
        "    axk.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mvdz41CI_1X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## (N) 各データ点（データフレームの行）ごとに正規化する\n",
        "\n",
        "### 正規化（normalization）\n",
        "\n",
        "$p$ 種類の特徴量がすべて数値のとき，$n$ 行あるデータフレームの各行を $p$ 次元のデータベクトル $\\boldsymbol{x}^{(i)}\\in\\mathbb{R}^p$（$i=1,\\dots,n$）と見なし，そのユークリッドノルム（$\\ell_2$ノルム，つまり原点からの距離）で割ります．\n",
        "\n",
        "👍 **長所**：データベクトル $\\boldsymbol{x}^{(i)}$ の方向が重要で，大きさ $\\|\\boldsymbol{x}^{(i)}\\|_2$ が重要でない場合に有効です．  \n",
        "👎 **短所**：他のスケーリングと異なり，特徴量（列ごと）ではなくデータごと（行ごと）に大きさが調整されるので，特徴量の範囲の目安がわかりません．  \n",
        "🎯 **使いどころ**：コサイン類似度を用いるデータ分類など，ベクトルの向きが重要なときに利用されます（例：TF-IDFや，大規模言語モデルの埋め込み表現を用いたテキスト分類など）．\n",
        "\n",
        "データベクトル $\\boldsymbol{x}^{(i)}$ を $\\ell_2$正規化した結果 $\\hat{\\boldsymbol{x}}^{(i)}$ は次式のように表せます．\n",
        "$$\\hat{\\boldsymbol{x}}^{(i)} =\\frac{\\boldsymbol{x}^{(i)}}{\\|\\boldsymbol{x}^{(i)}\\|_2} $$\n",
        "\n",
        "$\\hat{\\boldsymbol{x}}^{(i)}$ の$\\ell_2$ノルムは 1 です（$\\because \\|\\hat{\\boldsymbol{x}}^{(i)}\\|_2=\\|\\boldsymbol{x}^{(i)}\\|_2/\\|\\boldsymbol{x}^{(i)}\\|_2=1$）．\n",
        "\n",
        "---\n",
        "\n",
        "#### 正規化の前に中心化する利点\n",
        "\n",
        "データフレームの各特徴量が中心化済みの場合，$\\boldsymbol{x}^{(i)}$ の標本平均はゼロになります($\\frac{1}{n}\\sum_{i=1}^n\\boldsymbol{x}^{(i)}=\\boldsymbol{0}_p$)．\n",
        "つまり，$p$ 次元空間 $\\mathbb{R}^p$ において，$\\boldsymbol{x}^{(i)}$ を位置ベクトルとする $n$ 個のデータ点は，原点を中心に分布しています．\n",
        "そのような中心化済みのデータ集合は，データベクトル $\\boldsymbol{x}^{(i)}$（$i=1,\\dots,n$）の方向によってデータの同異がよく表されています．さらに正規化を施すことで，方向に基づくデータ分析が可能になります．"
      ],
      "metadata": {
        "id": "j9c6zIRo0WrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 正規化（normalization）---\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = Normalizer()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# sklearn を使わないで X_normalized を作りましょう．\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "#X_normalized = X.values / np.linalg.norm(X, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "zbEooN4dMFrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 中心化してから正規化する場合\n",
        "scaler = Normalizer()\n",
        "X_normalized = scaler.fit_transform(X - X.mean())"
      ],
      "metadata": {
        "id": "b-FhXXLvoR_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2つの特徴量を指定して，スケーリング前後の散布図を観察しましょう\n",
        "# 横軸と縦軸の特徴量を２つ選んでください\n",
        "feats = ['wt', 'mpg']  # ['wt', 'mpg', df['vs'].values] # ３つ目があれば色分けに使う\n",
        "\n",
        "dfXs = [(\"Before\", X[feats[:2]]),\n",
        "        (\"After (scaled)\", pd.DataFrame(X_normalized[:,df.columns.get_indexer(feats[:2])], columns=feats[:2]))]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(dfXs), figsize=(14, 5))\n",
        "fig.subplots_adjust(wspace=0.5)\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "for (_title, _df), _ax in zip(dfXs, axes):\n",
        "    _ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    sns.kdeplot(ax=_ax, data=_df, x=feats[0], y=feats[1], fill=True, alpha=0.1)\n",
        "    sns.scatterplot(ax=_ax, data=_df, x=feats[0], y=feats[1], s=60, hue=feats[2] if len(feats)==3 else None).set_title(_title)\n",
        "    axk = _ax.inset_axes([0, 1.02, 1, 0.2]) # [left, bottom, width, height]\n",
        "    sns.kdeplot(ax=axk, data=_df, x=feats[0], fill=True)\n",
        "    axk.axis('off')\n",
        "    axk = _ax.inset_axes([1.02, 0, 0.2, 1])\n",
        "    sns.kdeplot(ax=axk, data=_df, y=feats[1], fill=True)\n",
        "    axk.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M4IyPYjNMFv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYLBlXlImUoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_lgZr9PzmUs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ade4PpFYE9Hs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}